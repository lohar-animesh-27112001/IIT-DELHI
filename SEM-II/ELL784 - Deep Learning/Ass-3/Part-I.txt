import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

# ----- Hyperparameters -----
set_seed = lambda seed=1127: (torch.manual_seed(seed), torch.cuda.manual_seed_all(seed))
set_seed()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 64
LR = 5e-5
NUM_EPOCHS = 4

mean, std = (0.5,), (0.5,)
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
trainset = datasets.MNIST('../data/MNIST/', download=True, train=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
testset = datasets.MNIST('../data/MNIST/', download=True, train=False, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

image_size = 28
channel_size = 1
patch_size = 7
embed_size = 512
num_heads  = 8
num_layers = 3
mlp_dim    = 256
num_classes = 10
dropout    = 0.2

# ----- Patch Embedding -----
class PatchEmbedding(nn.Module):
    def __init__(self, image_size, patch_size, in_channels, embed_dim):
        super().__init__()
        assert image_size % patch_size == 0, "Image size must be divisible by patch size"
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) * (image_size // patch_size)
        patch_dim = in_channels * patch_size * patch_size
        self.proj = nn.Linear(patch_dim, embed_dim)

    def forward(self, x):
        # x: (B, C, H, W)
        B, C, H, W = x.shape
        # Split into patches: (B, C, H//p, p, W//p, p)
        x = x.view(B, C, H//self.patch_size, self.patch_size, W//self.patch_size, self.patch_size)
        # Rearrange to (B, num_patches, patch_dim)
        x = x.permute(0,2,4,1,3,5).contiguous()
        x = x.view(B, self.num_patches, -1)
        # Linear projection
        x = self.proj(x)
        return x

# ----- Multi-Head Self-Attention -----
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout):
        super().__init__()
        assert embed_dim % num_heads == 0, "Embedding dim must be divisible by num_heads"
        self.num_heads = num_heads
        self.head_dim  = embed_dim // num_heads
        # Q, K, V projections
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)
        # Output projection
        self.W_o = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.scale = self.head_dim ** -0.5

    def forward(self, x):
        # x: (B, N, embed_dim)
        B, N, E = x.size()
        # Linear projections
        Q = self.W_q(x)  # (B, N, E)
        K = self.W_k(x)
        V = self.W_v(x)
        # Reshape for multi-head: (B, heads, N, head_dim)
        Q = Q.view(B, N, self.num_heads, self.head_dim).transpose(1,2)
        K = K.view(B, N, self.num_heads, self.head_dim).transpose(1,2)
        V = V.view(B, N, self.num_heads, self.head_dim).transpose(1,2)
        # Scaled dot-product attention
        scores = (Q @ K.transpose(-2, -1)) * self.scale  # (B, heads, N, N)
        attn = scores.softmax(dim=-1)
        attn = self.dropout(attn)
        # Attend to V: (B, heads, N, head_dim)
        context = attn @ V
        # Concatenate heads: (B, N, E)
        context = context.transpose(1,2).contiguous().view(B, N, E)
        # Final linear projection
        out = self.W_o(context)
        return out

# ----- Transformer Encoder Layer -----
class TransformerEncoderLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.attn  = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        self.mlp   = nn.Sequential(
            nn.Linear(embed_dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, embed_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        # Multi-head attention with residual and pre-norm
        x = x + self.attn(self.norm1(x))
        # MLP with residual
        x = x + self.mlp(self.norm2(x))
        return x

# ----- Vision Transformer Model -----
class VisionTransformer(nn.Module):
    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads,
                 num_layers, mlp_dim, num_classes, dropout):
        super().__init__()
        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)
        # Learnable [CLS] token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        # Learnable positional embeddings
        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches+1, embed_dim))
        self.dropout = nn.Dropout(dropout)
        # Stack of Transformer encoder layers
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, dropout)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self.fc   = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        B = x.size(0)
        # Patch embedding (B, num_patches, embed_dim)
        x = self.patch_embed(x)
        # Prepare [CLS] token
          # (B,1,embed_dim)
        cls_tokens = self.cls_token.expand(B, -1, -1)
          # (B,1+num_patches,embed_dim)
        x = torch.cat((cls_tokens, x), dim=1)
        # Add positional embeddings
        x = x + self.pos_embed
        x = self.dropout(x)
        # Transformer layers
        for layer in self.layers:
            x = layer(x)
        # Classification head uses the [CLS] token output
          # (B, embed_dim)
        cls_out = self.norm(x[:, 0])
          # (B, num_classes)
        logits  = self.fc(cls_out)
        return F.log_softmax(logits, dim=1)

# Instantiate model
model = VisionTransformer(
    image_size=image_size,
    patch_size=patch_size,
    in_channels=channel_size,
    embed_dim=embed_size,
    num_heads=num_heads,
    num_layers=num_layers,
    mlp_dim=mlp_dim,
    num_classes=num_classes,
    dropout=dropout
).to(device)

criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

# (The training loop and evaluation below remain unchanged from the original code)
loss_hist = {"train_accuracy": [], "train_loss": []}
for epoch in range(1, NUM_EPOCHS+1):
    model.train()
    epoch_loss = 0.0
    y_true, y_pred = [], []
    for imgs, labels in trainloader:
        imgs, labels = imgs.to(device), labels.to(device)
        preds = model(imgs)
        loss = criterion(preds, labels)
        optimizer.zero_grad(); loss.backward(); optimizer.step()
        epoch_loss += loss.item()
        y_pred.extend(preds.argmax(dim=1).tolist())
        y_true.extend(labels.tolist())
    acc = 100.0 * sum(int(p==t) for p,t in zip(y_pred, y_true)) / len(y_true)
    loss_hist["train_loss"].append(epoch_loss)
    loss_hist["train_accuracy"].append(acc)
    print(f"Epoch {epoch}: Train loss={epoch_loss:.4f}, accuracy={acc:.4f}%")

with torch.no_grad():
    model.eval()
    y_true, y_pred = [], []
    for imgs, labels in testloader:
        imgs, labels = imgs.to(device), labels.to(device)
        preds = model(imgs)
        y_pred.extend(preds.argmax(dim=1).tolist())
        y_true.extend(labels.tolist())
    total = len(y_true)
    correct = sum(int(p==t) for p,t in zip(y_pred, y_true))
    print(f"Test Accuracy: {100.0*correct/total:.2f}% ({correct}/{total})")
