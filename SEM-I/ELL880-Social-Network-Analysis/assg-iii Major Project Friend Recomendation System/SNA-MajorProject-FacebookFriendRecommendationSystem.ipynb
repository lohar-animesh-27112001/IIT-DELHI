{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxXZImtNFX0TGMy2Y0bAn0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b321a5d4cae942988d3e7a2aac6d25e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dea750c9de1f456bb22dff306aa22ca0","IPY_MODEL_c56c5ae0c3894443bc534e3e3b0789e9","IPY_MODEL_9e060b3b60ed40c7941a64cd4d855aa8"],"layout":"IPY_MODEL_35b3d0cc8c3043d7941d4dc491a74873"}},"dea750c9de1f456bb22dff306aa22ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fceedf15bf6c4f86aee70d22c2a31846","placeholder":"​","style":"IPY_MODEL_c4ab06ebb2c74c8cba18801318f4fd50","value":"Computing transition probabilities: 100%"}},"c56c5ae0c3894443bc534e3e3b0789e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_283c334808df4ef1a3dd09f00d331638","max":3959,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5ce2021d26645c0a1e2ccbacb559d34","value":3959}},"9e060b3b60ed40c7941a64cd4d855aa8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea032d3fc734484eae49b0e8333ae33b","placeholder":"​","style":"IPY_MODEL_6dbee7b4ceff41dfafa0c35da4900f09","value":" 3959/3959 [01:17&lt;00:00, 293.24it/s]"}},"35b3d0cc8c3043d7941d4dc491a74873":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fceedf15bf6c4f86aee70d22c2a31846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4ab06ebb2c74c8cba18801318f4fd50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"283c334808df4ef1a3dd09f00d331638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ce2021d26645c0a1e2ccbacb559d34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea032d3fc734484eae49b0e8333ae33b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dbee7b4ceff41dfafa0c35da4900f09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Algorithms used: ***SVM, RandomForestClassifier, XGBoost, Node2vec***\n","## **For SVM** : got 45 - 50 % accuracy\n","## **For RandomForesrtClassifier** : got 80 - 85 % accuracy\n","## **For XGBoost** : got 92 - 95 % accuracy\n","## **For XGBoost + Node2vec** : got 94 - 98 % accuracy\n","\n","## **Dataset Link** : https://snap.stanford.edu/data/ego-Facebook.html"],"metadata":{"id":"byR4eLOyBp11"}},{"cell_type":"markdown","source":["# **Algorithm used : SVM**"],"metadata":{"id":"qYFPlXf2WyUR"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"pr6ijOopXTwq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Advantages of Support Vector Machines (SVM)**\n","\n","#### **1. Simplicity**\n","- SVM is **straightforward to implement** for binary classification tasks, making it an accessible choice for many applications.\n","- When using the **linear kernel**, the algorithm trains efficiently, ensuring quick setup and deployment.\n","\n","#### **2. Efficiency**\n","- SVM is particularly effective with **small to medium-sized datasets**, delivering accurate and reliable results without requiring excessive computational resources."],"metadata":{"id":"xcoja3F2XeQ-"}},{"cell_type":"code","source":["def load_features(file_path):\n","    data = pd.read_csv(file_path, header=None, delim_whitespace=True).values\n","    return data\n","\n","def load_edges(file_path):\n","    data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n","    return data.values\n","\n","def align_features(egofeat, feat):\n","    # Check if dimensions match\n","    if egofeat.shape[1] != feat.shape[1]:\n","        min_cols = min(egofeat.shape[1], feat.shape[1])\n","        egofeat = egofeat[:, :min_cols]\n","        feat = feat[:, :min_cols]\n","    return egofeat, feat\n","\n","def prepare_data(edges_file, feat_file, egofeat_file):\n","    edges = load_edges(edges_file)\n","    feat = load_features(feat_file)\n","    egofeat = load_features(egofeat_file)\n","    egofeat, feat = align_features(egofeat, feat)\n","    features = np.vstack([egofeat, feat])\n","    labels = np.array([1 if i % 2 == 0 else 0 for i in range(features.shape[0])])\n","    return features, labels"],"metadata":{"id":"cUrPrgdJYcbo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Support Vector Machine (SVM) - Explanation**\n","\n","---\n","\n","#### **Objective Function**\n","The objective of SVM is to minimize the following function:\n","\n","$$\n","\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n","$$\n","\n","where:\n","- $(\\mathbf{w})$ is the weight vector (defining the hyperplane).\n","- $(b)$ is the bias term.\n","- $(\\|\\mathbf{w}\\|^2)$ is the squared norm of the weight vector, which is minimized to maximize the margin.\n","\n","---\n","\n","#### **Constraints**\n","For linearly separable data, the SVM must satisfy:\n","\n","$$\n","y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n","$$\n","\n","---\n","\n","#### **Soft Margin SVM (Non-Separable Data)**\n","For non-linearly separable data, a slack variable $(\\xi_i)$ is introduced to allow some misclassifications. The optimization problem becomes:\n","\n","$$\n","\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n","$$\n","\n","subject to:\n","\n","$$\n","y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n","$$\n","\n","where:\n","- $(\\xi_i)$ measures the misclassification for the sample.\n","- \\(C\\) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\n","\n","---\n","\n","#### **Kernel SVM**\n","When data is not linearly separable, it is transformed into a higher-dimensional space using a kernel function. \\$(K(\\mathbf{x}_i, \\mathbf{x}_j))$ The decision boundary is then computed as:\n","\n","$$\n","f(\\mathbf{x}) = \\text{sign}\\left( \\sum_{i=1}^{n} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\right)\n","$$\n","\n","---\n","\n","#### **Decision Boundary**\n","The decision function for SVM is given by:\n","\n","$$\n","f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b\n","$$\n","\n","#### **Margin**\n","The margin is defined as:\n","\n","$$\n","\\text{Margin} = \\frac{2}{\\|\\mathbf{w}\\|}\n","$$\n","\n","Maximizing the margin ensures better generalization of the classifier.\n"],"metadata":{"id":"Q7O60XjHZgqk"}},{"cell_type":"code","source":["def main():\n","    edges_file = \"./facebook_data/0.edges\"\n","    feat_file = \"./facebook_data/0.feat\"\n","    egofeat_file = \"./facebook_data/0.egofeat\"\n","\n","    # Load and prepare data\n","    features, labels = prepare_data(edges_file, feat_file, egofeat_file)\n","\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","    # Train SVM\n","    clf = SVC(kernel=\"linear\", random_state=42)\n","    clf.fit(X_train, y_train)\n","\n","    # Test the model\n","    y_pred = clf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f\"Accuracy: {accuracy*100:.2f} %\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xI6kCNb-ZXzW","executionInfo":{"status":"ok","timestamp":1732599860663,"user_tz":-330,"elapsed":2270,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"3ae3f919-1aa3-4415-9bd3-5dcf936f9ffc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-95ae4f63b8f6>:6: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n","<ipython-input-14-95ae4f63b8f6>:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  data = pd.read_csv(file_path, header=None, delim_whitespace=True).values\n","<ipython-input-14-95ae4f63b8f6>:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  data = pd.read_csv(file_path, header=None, delim_whitespace=True).values\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 47.14 %\n"]}]},{"cell_type":"markdown","source":["### Disadvantages of the Current Implementation : **Getting very low accuracy**\n","\n","1. **Dummy Labels**  \n","   - The labels are randomly generated (1 for even indices, 0 for odd), which doesn't reflect real-world data.  \n","   - This approach is a heuristic and does not utilize meaningful labels from the dataset.  \n","   - **Solution:** Replace this heuristic with meaningful labels, such as using the circles from the `.circles` file for classification tasks.\n","\n","2. **No Use of Edges**  \n","   - While the edge data is loaded, it is not utilized in the model.  \n","   - This ignores valuable information about the graph's structure, such as relationships between nodes.  \n","   - **Solution:** Incorporate edge data into the model to capture relational information, such as adjacency matrices or features derived from the graph topology.\n","\n","3. **Feature Engineering**  \n","   - The implementation does not include graph-based features like:\n","     - Node degree (number of connections for each node).\n","     - Clustering coefficient (measure of local connectivity around a node).\n","     - Neighbor-based features (e.g., average or sum of neighbor features).  \n","   - **Solution:** Perform additional feature engineering to extract graph-based features that better represent the data and improve classification performance."],"metadata":{"id":"qiXSBWqSfJFx"}},{"cell_type":"markdown","source":["# **Algorithm used : Random Forest Classifier**"],"metadata":{"id":"oRuizYFgf133"}},{"cell_type":"code","source":["import networkx as nx\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"SFqT3BBUgO4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **1. Load the Graph (`load_graph`)**\n","#### **Purpose:**\n","Constructs a graph from an edge list file.\n","\n","#### **Input:**\n","- File containing edges, where each line represents an edge (e.g., `u v`).\n","\n","#### **Output:**\n","- A graph \\( G \\) represented as a NetworkX object."],"metadata":{"id":"fFhEapTwg7xX"}},{"cell_type":"code","source":["def load_graph(edges_file):\n","    G = nx.Graph()\n","    with open(edges_file, 'r') as f:\n","        for line in f:\n","            u, v = map(int, line.strip().split())\n","            G.add_edge(u, v)\n","    return G"],"metadata":{"id":"v0QOxy7Dg8kJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2. Load Features (`load_features`)**\n","#### **Purpose:**\n","Loads features for each node from the feature file (`feat_file`) and ego features (`egofeat_file`) and combines them.\n","\n","#### **Input:**\n","- `feat_file`: Node features.\n","- `egofeat_file`: Ego (self) features.\n","\n","#### **Output:**\n","- A single dataframe containing features for all nodes."],"metadata":{"id":"48-WrjEeiNbx"}},{"cell_type":"code","source":["def load_features(feat_file, egofeat_file):\n","    features = pd.read_csv(feat_file, header=None, delimiter=' ')\n","    egofeat = pd.read_csv(egofeat_file, header=None, delimiter=' ')\n","    return pd.concat([egofeat, features], ignore_index=True)"],"metadata":{"id":"DlZH43l8iWM5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **3. Generate Training Data (`generate_training_data`)**\n","#### **Purpose:**\n","Creates labeled data for supervised link prediction.\n","\n","#### **Steps:**\n","1. **Edges:** Extract all edges from the graph \\( G \\) using `G.edges()`.\n","2. **Non-Edges:** Use `nx.non_edges()` to sample an equal number of non-existent edges.\n","3. **Labels:**  \n","   - Assign \\( 1 \\) to actual edges.  \n","   - Assign \\( 0 \\) to sampled non-edges.\n","4. **Feature Extraction:**  \n","   - For each edge or non-edge \\( (u, v) \\), concatenate the feature vectors of nodes \\( u \\) and \\( v \\).  \n","   - Combine all edge and non-edge features into \\( X \\), and labels into \\( y \\).\n","\n","#### **Algorithm Used:**\n","- Random sampling for non-edges.\n","- Feature concatenation for supervised learning.\n","\n","#### **Output:**\n","- \\( X \\): Feature matrix (each row corresponds to an edge/non-edge pair).\n","- \\( y \\): Labels indicating whether the pair represents an edge (\\( 1 \\)) or non-edge (\\( 0 \\)).\n"],"metadata":{"id":"d1_OqhMyjg70"}},{"cell_type":"code","source":["def generate_training_data(G, features):\n","    edges = list(G.edges())\n","    non_edges = list(nx.non_edges(G))\n","    np.random.shuffle(non_edges)\n","\n","    labels = [1] * len(edges) + [0] * len(non_edges[:len(edges)])\n","\n","    def extract_features(u, v):\n","        return np.concatenate([features.iloc[u].values, features.iloc[v].values])\n","\n","    edge_features = [extract_features(u, v) for u, v in edges]\n","    non_edge_features = [extract_features(u, v) for u, v in non_edges[:len(edges)]]\n","\n","    X = np.array(edge_features + non_edge_features)\n","    y = np.array(labels)\n","    return X, y"],"metadata":{"id":"3JkPqouGigOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **4. Train and Evaluate Model (`train_and_evaluate`)**\n","#### **Purpose:**\n","Splits the data into training and test sets, trains a classifier, and evaluates its performance.\n","\n","#### **Algorithm Used:**\n","- **Model:** Random Forest Classifier (ensemble method).  \n","  - Random Forest constructs multiple decision trees and outputs the majority vote (classification) or average (regression).\n","\n","#### **Key Parameters:**\n","- `n_estimators=100`: The number of decision trees in the forest.\n","\n","#### **Steps:**\n","1. **Train-Test Split:**  \n","   - Split the data:  \n","     - 80% of the data for training.  \n","     - 20% of the data for testing.\n","2. **Evaluation Metrics:**  \n","   - **Accuracy:** Proportion of correctly predicted samples.  \n","   - **Classification Report:** Includes precision, recall, F1-score, etc."],"metadata":{"id":"i8tT7XSBjxKA"}},{"cell_type":"code","source":["def train_and_evaluate(X, y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    model = RandomForestClassifier(n_estimators=100, random_state=42)\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f\"Accuracy: {accuracy * 100:.2f} %\")\n","    print(classification_report(y_test, y_pred))\n","    return model"],"metadata":{"id":"glmH3LVvijeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Advantages**\n","\n","#### **1. Robustness to Overfitting**\n","Random Forest reduces overfitting by averaging the results of multiple decision trees.  \n","##### Formula for aggregation:\n","$[\n","\\hat{y} = \\text{Majority Vote or Mean}\\left(\\text{predictions from } T_1, T_2, \\dots, T_n\\right)\n","]$\n","- $( \\hat{y} )$: Final prediction.  \n","- $( T_i )$: Individual decision tree in the forest.\n","\n","---\n","\n","#### **2. Handles High-Dimensional Data**\n","Random Forest can handle datasets with a large number of features, as it selects a subset of features for each split:  \n","##### Formula:\n","$[\n","\\text{Features Per Split} =\n","\\begin{cases}\n","\\sqrt{d} & \\text{for classification} \\\\\n","d/3 & \\text{for regression}\n","\\end{cases}\n","]$\n","- \\( d \\): Total number of features.\n","\n","---\n","\n","#### **3. Resilient to Missing and Noisy Data**\n","The ensemble nature ensures stability even if some data is noisy or missing.\n","\n","---\n","\n","#### **4. Feature Importance**\n","Random Forest provides a mechanism to measure feature importance:  \n","##### Formula:\n","\\$[\n","\\text{Feature Importance} = \\frac{\\sum (\\text{Decrease in Impurity for Splits on Feature})}{\\text{Total Trees}}\n","\\$]\n","\n","---\n","\n","#### **5. Non-Parametric**\n","Random Forest does not make any assumptions about the data distribution (e.g., linearity or normality).\n"],"metadata":{"id":"9yOqCkV5kbo0"}},{"cell_type":"markdown","source":["### **Getting better accuracy with single dataset**"],"metadata":{"id":"DYB9lXFllVse"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Paths to files\n","    edges_file = \"./facebook_data/0.edges\"\n","    feat_file = \"./facebook_data/0.feat\"\n","    egofeat_file = \"./facebook_data/0.egofeat\"\n","\n","    # Load graph and features\n","    G = load_graph(edges_file)\n","    features = load_features(feat_file, egofeat_file)\n","\n","    # Generate training data\n","    X, y = generate_training_data(G, features)\n","\n","    # Train and evaluate the model\n","    model = train_and_evaluate(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6PUJwzbaaX2","executionInfo":{"status":"ok","timestamp":1732599888414,"user_tz":-330,"elapsed":4317,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"090d2242-e564-4c59-8db5-6c68256cb53a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 82.24 %\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.85      0.82       490\n","           1       0.85      0.79      0.82       518\n","\n","    accuracy                           0.82      1008\n","   macro avg       0.82      0.82      0.82      1008\n","weighted avg       0.82      0.82      0.82      1008\n","\n"]}]},{"cell_type":"markdown","source":["# **Algorithm used : XGBoost Classifier**\n","\n","XGBoost is an ensemble learning algorithm based on **gradient boosting**.\n","\n","**Objective**: Minimize a loss function using a series of decision trees.\n","\n","The loss function $(L(\\phi))$ is defined as:\n","\n","$[\n","L(\\phi) = \\sum_{i=1}^{n} \\ell(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n","]$\n","\n","Where:\n","- $(\\ell)$: Loss function (e.g., logistic loss for classification).\n","- $(\\hat{y}_i)$: Predicted label for the \\(i\\)-th data point.\n","- $(\\Omega(f_k))$: Regularization term for each tree \\(f_k\\).\n","- $(K)$: Total number of trees in the ensemble."],"metadata":{"id":"4K3ucjTPnUL-"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import networkx as nx\n","from xgboost import XGBClassifier\n","from sklearn.preprocessing import LabelEncoder\n","from collections import defaultdict"],"metadata":{"id":"FmoK81_joH6U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **load_edges(file_path) :**\n","Loads the edge list from a file, assuming edges are stored in a space-separated format.\n","Output: A DataFrame with source and target columns, representing graph edges."],"metadata":{"id":"3M3zbqNuoxTt"}},{"cell_type":"code","source":["def load_edges(file_path):\n","    edges = pd.read_csv(file_path, sep=\" \", header=None, names=[\"source\", \"target\"])\n","    return edges"],"metadata":{"id":"_ycatIT1o9-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **load_features(feat_path, ego_feat_path) :**\n","Combines node features (feat_file) and ego node features (egofeat_file).\n","Output: A concatenated DataFrame of features."],"metadata":{"id":"EIVY6GSTpBmr"}},{"cell_type":"code","source":["def load_features(feat_path, ego_feat_path):\n","    features = pd.read_csv(feat_path, sep=\" \", header=None)\n","    ego_features = pd.read_csv(ego_feat_path, sep=\" \", header=None)\n","    return pd.concat([features, ego_features], axis=0).reset_index(drop=True)"],"metadata":{"id":"jm6dDWfDpMZj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **load_circles(circle_path) :**\n","Loads circle (community) labels. Each line in the file represents a circle with its associated nodes.\n","Output: A list of lists, where each inner list contains nodes belonging to a circle."],"metadata":{"id":"3ReoYdLZpgyI"}},{"cell_type":"code","source":["def load_circles(circle_path):\n","    circles = []\n","    with open(circle_path, 'r') as f:\n","        for line in f:\n","            nodes = line.strip().split('\\t')[1:]  # Exclude the circle name\n","            circles.append(nodes)\n","    return circles"],"metadata":{"id":"Z1BbSqqkpP9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **prepare_dataset(files) :**\n","Processes multiple data files (edges, features, circles) and constructs a graph using NetworkX."],"metadata":{"id":"gTZJh7lBp6-d"}},{"cell_type":"code","source":["def prepare_dataset(files):\n","    G = nx.Graph()\n","    features = []\n","    labels = []\n","\n","    for file in files:\n","        edges_file = f\"{file}.edges\"\n","        feat_file = f\"{file}.feat\"\n","        ego_feat_file = f\"{file}.egofeat\"\n","        circle_file = f\"{file}.circles\"\n","\n","        edges = load_edges(edges_file)\n","        G.add_edges_from(edges.values)\n","\n","        node_features = load_features(feat_file, ego_feat_file)\n","        features.append(node_features)\n","\n","        circles = load_circles(circle_file)\n","        for circle in circles:\n","            for node in circle:\n","                labels.append((int(node), len(circles)))\n","\n","    features = pd.concat(features, axis=0).reset_index(drop=True)\n","    return G, features, labels"],"metadata":{"id":"29QwRHJ-bRo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Using the files**"],"metadata":{"id":"KoxLlMyzqj6p"}},{"cell_type":"code","source":["files = ['./facebook_data/0', './facebook_data/107', './facebook_data/348',\n","         './facebook_data/414', './facebook_data/686', './facebook_data/698',\n","         './facebook_data/1684', './facebook_data/1912', './facebook_data/3437',\n","         './facebook_data/3980']"],"metadata":{"id":"Ocj8nSc7qXME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G, features, labels = prepare_dataset(files)"],"metadata":{"id":"lIVGztjXqYVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Adding some extra features**"],"metadata":{"id":"_ekU4CjqqtbI"}},{"cell_type":"code","source":["def add_graph_features(G, features_df):\n","    degrees = dict(G.degree())\n","    clustering_coeffs = nx.clustering(G)\n","\n","    features_df['degree'] = features_df.index.map(degrees)\n","    features_df['clustering_coeff'] = features_df.index.map(clustering_coeffs)\n","    return features_df\n","\n","features = add_graph_features(G, features)"],"metadata":{"id":"ryRc_sWvbk2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Getting accuracy more than : 95 %**"],"metadata":{"id":"H--7xkGRq8bz"}},{"cell_type":"code","source":["# Create training data\n","node_labels = {node: label for node, label in labels}\n","X = features.values\n","y = np.array([node_labels.get(i, -1) for i in range(len(features))])  # Assign -1 for unlabeled nodes\n","\n","# Filter out unlabeled nodes\n","X = X[y != -1]\n","y = y[y != -1]\n","\n","# --- Apply Label Encoding ---\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","# --- End of Label Encoding ---\n","\n","# Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train an XGBoost model\n","model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy*100:.2f} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrBXAKZ0biR7","executionInfo":{"status":"ok","timestamp":1732599922923,"user_tz":-330,"elapsed":2680,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"18be560c-24f7-4852-8fa8-05fa7230380d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:45:20] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 96.36 %\n"]}]},{"cell_type":"markdown","source":["# **Using Neighbor-based Collaborative Filtering - A basic algorithm**"],"metadata":{"id":"AEdi9eW9tJGj"}},{"cell_type":"code","source":["def load_facebook_data(folder_path):\n","    G = nx.Graph()\n","    features = {}\n","    for file in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, file)\n","        if file.endswith(\".edges\"):\n","            with open(file_path, 'r') as f:\n","                for line in f:\n","                    node1, node2 = map(int, line.split())\n","                    G.add_edge(node1, node2)\n","        elif file.endswith(\".feat\"):\n","            with open(file_path, 'r') as f:\n","                for line in f:\n","                    data = list(map(int, line.split()))\n","                    node_id, node_features = data[0], data[1:]\n","                    features[node_id] = node_features\n","    return G, features"],"metadata":{"id":"VZsfUCMG1ZCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recommend_friends(graph, node, top_k=5):\n","    scores = defaultdict(int)\n","    for neighbor in graph.neighbors(node):\n","        for potential_friend in graph.neighbors(neighbor):\n","            if potential_friend != node and not graph.has_edge(node, potential_friend):\n","                scores[potential_friend] += 1\n","    recommendations = sorted(scores.items(), key=lambda x: -x[1])[:top_k]\n","    return recommendations"],"metadata":{"id":"nIlZpQH01eQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Printing Top 10 reccomendation for 3100 node**"],"metadata":{"id":"4K35W9OItVm1"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    dataset_folder = \"./facebook_data\"\n","    print(\"Loading dataset...\")\n","    graph, node_features = load_facebook_data(dataset_folder)\n","    print(f\"Graph loaded with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n","    test_node = 3100\n","    print(f\"Generating friend recommendations for node {test_node}...\")\n","    recommendations = recommend_friends(graph, test_node, top_k=10)\n","    print(\"Top recommendations:\")\n","    for friend, score in recommendations:\n","        print(f\"Node {friend} (Score: {score})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPMbnL9l1hDs","executionInfo":{"status":"ok","timestamp":1732599934055,"user_tz":-330,"elapsed":1028,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"8839797d-f8ad-4b14-9537-35e3499cebcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Graph loaded with 3959 nodes and 84243 edges.\n","Generating friend recommendations for node 3100...\n","Top recommendations:\n","Node 2775 (Score: 15)\n","Node 3412 (Score: 14)\n","Node 3001 (Score: 13)\n","Node 3051 (Score: 13)\n","Node 3086 (Score: 13)\n","Node 2728 (Score: 13)\n","Node 3113 (Score: 12)\n","Node 3207 (Score: 12)\n","Node 2739 (Score: 12)\n","Node 3076 (Score: 12)\n"]}]},{"cell_type":"markdown","source":["# **Using node2vec embeddings and XGBoost machine learning classifier**"],"metadata":{"id":"GGaMMLS9tyP_"}},{"cell_type":"code","source":["pip install node2vec"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wgrrb3MTvB11","executionInfo":{"status":"ok","timestamp":1732599942888,"user_tz":-330,"elapsed":4881,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"10bf7f40-9b65-4ae1-a2d2-285c2c86fab0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: node2vec in /usr/local/lib/python3.10/dist-packages (0.5.0)\n","Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.3)\n","Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.4.2)\n","Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (3.4.2)\n","Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.26.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.66.6)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.16.0)\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import networkx as nx\n","import numpy as np\n","from node2vec import Node2Vec\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from xgboost import XGBClassifier\n","from collections import defaultdict"],"metadata":{"id":"FSPKwVmItwf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Load edges and features**"],"metadata":{"id":"iVG-ijEdvMcP"}},{"cell_type":"code","source":["def load_facebook_data(folder_path):\n","    G = nx.Graph()\n","    features = {}\n","    for file in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, file)\n","        if file.endswith(\".edges\"):\n","            with open(file_path, 'r') as f:\n","                for line in f:\n","                    node1, node2 = map(int, line.split())\n","                    G.add_edge(node1, node2)\n","        elif file.endswith(\".feat\"):\n","            with open(file_path, 'r') as f:\n","                for line in f:\n","                    data = list(map(int, line.split()))\n","                    node_id, node_features = data[0], data[1:]\n","                    features[node_id] = node_features\n","    return G, features"],"metadata":{"id":"w45hRyfVvRiy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Generate node embeddings using node2vec**"],"metadata":{"id":"M7ZoNcQCvVMd"}},{"cell_type":"code","source":["def generate_node_embeddings(G, dimensions=128, walk_length=80, num_walks=10, window=10):\n","    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=4)\n","    model = node2vec.fit(window=window)\n","    embeddings = {node: model.wv.get_vector(str(node)) for node in G.nodes()}\n","    return embeddings"],"metadata":{"id":"9P2l4PI0vdq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Prepare training data for friend recommendation**"],"metadata":{"id":"l9Xnbuxmvfb-"}},{"cell_type":"code","source":["def prepare_training_data(G, embeddings, top_k=10):\n","    features = []\n","    labels = []\n","    for node in G.nodes():\n","        node_embedding = embeddings[node]\n","        neighbors = list(G.neighbors(node))\n","        for neighbor in neighbors:\n","            neighbor_embedding = embeddings[neighbor]\n","            features.append(np.concatenate([node_embedding, neighbor_embedding]))\n","            labels.append(1)\n","        non_neighbors = [potential_friend for potential_friend in G.nodes() if not G.has_edge(node, potential_friend)]\n","        np.random.shuffle(non_neighbors)\n","        for non_friend in non_neighbors[:len(neighbors)]:\n","            non_friend_embedding = embeddings[non_friend]\n","            features.append(np.concatenate([node_embedding, non_friend_embedding]))\n","            labels.append(0)\n","    return np.array(features), np.array(labels)"],"metadata":{"id":"ronamdW0vtRn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Train the recommendation model (XGBoost)**"],"metadata":{"id":"P-TLLyAXvw5d"}},{"cell_type":"code","source":["def train_recommendation_model(X, y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    model = XGBClassifier(n_estimators=100, random_state=42)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f\"Accuracy: {accuracy * 100:.2f} %\")\n","    return model"],"metadata":{"id":"0B54qwROv5Ai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Recommend friends for a given node**"],"metadata":{"id":"r5BWV-IpwBxN"}},{"cell_type":"code","source":["def recommend_friends(graph, node, embeddings, model, top_k=5):\n","    node_embedding = embeddings[node]\n","    scores = []\n","\n","    for potential_friend in graph.nodes():\n","        if potential_friend != node and not graph.has_edge(node, potential_friend):\n","            potential_friend_embedding = embeddings[potential_friend]\n","            feature_vector = np.concatenate([node_embedding, potential_friend_embedding]).reshape(1, -1)\n","            score = model.predict_proba(feature_vector)[0][1]\n","            scores.append((potential_friend, score))\n","    recommendations = sorted(scores, key=lambda x: -x[1])[:top_k]\n","    return recommendations"],"metadata":{"id":"jFoIFh4fwGJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Load data\n","    dataset_folder = \"./facebook_data\"\n","    print(\"Loading dataset...\")\n","    graph, node_features = load_facebook_data(dataset_folder)\n","    print(f\"Graph loaded with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n","\n","    # Generate node embeddings\n","    print(\"Generating node embeddings using node2vec...\")\n","    embeddings = generate_node_embeddings(graph)\n","\n","    # Prepare training data\n","    print(\"Preparing training data...\")\n","    X, y = prepare_training_data(graph, embeddings)\n","\n","    # Train the recommendation model\n","    print(\"Training recommendation model...\")\n","    model = train_recommendation_model(X, y)\n","\n","    # Generate recommendations for a test node\n","    test_node = 3100\n","    print(f\"Generating friend recommendations for node {test_node}...\")\n","    recommendations = recommend_friends(graph, test_node, embeddings, model, top_k=10)\n","\n","    # Display recommendations\n","    print(\"Top recommendations:\")\n","    for friend, score in recommendations:\n","        print(f\"Node {friend} (Score: {score})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361,"referenced_widgets":["b321a5d4cae942988d3e7a2aac6d25e6","dea750c9de1f456bb22dff306aa22ca0","c56c5ae0c3894443bc534e3e3b0789e9","9e060b3b60ed40c7941a64cd4d855aa8","35b3d0cc8c3043d7941d4dc491a74873","fceedf15bf6c4f86aee70d22c2a31846","c4ab06ebb2c74c8cba18801318f4fd50","283c334808df4ef1a3dd09f00d331638","d5ce2021d26645c0a1e2ccbacb559d34","ea032d3fc734484eae49b0e8333ae33b","6dbee7b4ceff41dfafa0c35da4900f09"]},"id":"DnI9Mo9usSTv","executionInfo":{"status":"ok","timestamp":1732598940691,"user_tz":-330,"elapsed":393421,"user":{"displayName":"ANIMESH LOHAR","userId":"09287890823555865413"}},"outputId":"40fe29e9-3af9-4ad9-eeff-d2f6a2e0b88d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Graph loaded with 3959 nodes and 84243 edges.\n","Generating node embeddings using node2vec...\n"]},{"output_type":"display_data","data":{"text/plain":["Computing transition probabilities:   0%|          | 0/3959 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b321a5d4cae942988d3e7a2aac6d25e6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Preparing training data...\n","Training recommendation model...\n","Accuracy: 97.68 %\n","Generating friend recommendations for node 3100...\n","Top recommendations:\n","Node 3379 (Score: 0.984081506729126)\n","Node 2775 (Score: 0.9827460050582886)\n","Node 2976 (Score: 0.9809519648551941)\n","Node 2748 (Score: 0.9791770577430725)\n","Node 3273 (Score: 0.9790619611740112)\n","Node 2739 (Score: 0.9790334105491638)\n","Node 2742 (Score: 0.9787536859512329)\n","Node 2692 (Score: 0.978733241558075)\n","Node 3177 (Score: 0.9766011238098145)\n","Node 3106 (Score: 0.9724658131599426)\n"]}]}]}